Towards Ultradense Contextual Embeddings by Distribution-Based Orthogonal Transformation
----

Re-implementation for my bachelor thesis project (thesis defense in April 2022). 

Project Description
----
Embeddings are widely used in natural language processing tasks. But one concern is that embeddings from existing language models are dense and high-dimensional, which is difficult for people to interpret. 

In this work, we propose a distribution-based method to identify ultradense subspace from contextualized embedding space. We apply orthogonal transformations to original contextual embeddings and extract task-related information. 

We also put forward a novel perspective on contextual embeddings. Specifically, we view embeddings of the same token in different contexts as samples of one certain normal distribution. 

We employ the Wasserstein Distance to examine distances between those distributions to measure differences between embeddings.
